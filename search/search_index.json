{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Underpass documentation","text":""},{"location":"#whats-underpass","title":"What's Underpass?","text":""},{"location":"#underpass-is-a-customizable-data-engine-that-processes-mapping-data","title":"Underpass is a customizable data engine that processes mapping data.","text":"<p>It can update a local copy of the OSM database in near real-time, and provides customizable statistics and validation reports. It is designed to be high performance on modest hardware.</p> <p></p>"},{"location":"API/Python/","title":"Python","text":""},{"location":"API/Python/#call-underpass-functions-using-python-c-binding","title":"Call Underpass functions using Python C++ binding","text":""},{"location":"API/Python/#import-underpass-package","title":"Import Underpass package","text":"<pre><code>    import underpass as u\n</code></pre>"},{"location":"API/Python/#validate-osm-change","title":"Validate OSM Change","text":"<pre><code>    with open(\"building.osc\", 'r') as file:\n        data = file.read().rstrip()\n\n    validator = u.Validate()\n    result = validator.checkOsmChange(data, \"building\")\n</code></pre>"},{"location":"API/PythonDB/","title":"Python (DB)","text":""},{"location":"API/PythonDB/#get-data-from-the-underpass-db-using-python","title":"Get data from the Underpass DB using Python","text":""},{"location":"API/PythonDB/#connect-to-the-database","title":"Connect to the database","text":"<pre><code>from api.db import UnderpassDB`\ndb = UnderpassDB(\"postgresql://localhost/underpass\")\ndb.connect()\n</code></pre>"},{"location":"API/PythonDB/#get-raw-data","title":"Get raw data","text":"<pre><code>from api import raw\nrawer = raw.Raw(db)\n</code></pre>"},{"location":"API/PythonDB/#get-polygons","title":"Get polygons","text":"<pre><code>polygons = rawer.getPolygons( \n    area = \"-180 90,180 90, 180 -90, -180 -90,-180 90\",\n    tags = \"building=yes\",\n    hashtag = \"\",\n    dateFrom = \"\",\n    dateTo = \"\",\n    page = 0\n)\n</code></pre>"},{"location":"API/PythonDB/#get-lines","title":"Get lines","text":"<pre><code>lines = rawer.getLines( \n    area = \"-180 90,180 90, 180 -90, -180 -90,-180 90\",\n    tags = \"highway\",\n    hashtag = \"\",\n    dateFrom = \"\",\n    dateTo = \"\",\n    page = 0\n)\n</code></pre>"},{"location":"API/PythonDB/#get-nodes","title":"Get nodes","text":"<pre><code>nodes = rawer.getNodes( \n    area = \"-180 90,180 90, 180 -90, -180 -90,-180 90\",\n    tags = \"amenity\",\n    hashtag = \"\",\n    dateFrom = \"\",\n    dateTo = \"\",\n    page = 0\n)\n</code></pre>"},{"location":"API/PythonDB/#get-all-together-polygons-lines-and-nodes","title":"Get all together (polygons, lines and nodes)","text":"<pre><code>nodes = rawer.getAll( \n    area = \"-180 90,180 90, 180 -90, -180 -90,-180 90\",\n    tags = \"building\",\n    hashtag = \"\",\n    dateFrom = \"\",\n    dateTo = \"\",\n    page = 0\n)\n</code></pre>"},{"location":"API/PythonDB/#get-list-of-polygons","title":"Get list of polygons","text":"<pre><code>polygons = rawer.getPolygonsList( \n    area = \"-180 90,180 90, 180 -90, -180 -90,-180 90\",\n    tags = \"building\",\n    hashtag = \"\",\n    dateFrom = \"\",\n    dateTo = \"\",\n    page = 0\n)\n</code></pre>"},{"location":"API/PythonDB/#get-list-of-lines","title":"Get list of lines","text":"<pre><code>lines = rawer.getLinesList( \n    area = \"-180 90,180 90, 180 -90, -180 -90,-180 90\",\n    tags = \"building\",\n    hashtag = \"\",\n    dateFrom = \"\",\n    dateTo = \"\",\n    page = 0\n)\n</code></pre>"},{"location":"API/PythonDB/#get-list-of-nodes","title":"Get list of nodes","text":"<pre><code>nodes = rawer.getNodesList( \n    area = \"-180 90,180 90, 180 -90, -180 -90,-180 90\",\n    tags = \"building\",\n    hashtag = \"\",\n    dateFrom = \"\",\n    dateTo = \"\",\n    page = 0\n)\n</code></pre>"},{"location":"API/PythonDB/#get-list-of-all-together-polygons-lines-and-nodes","title":"Get list of all together (polygons, lines and nodes)","text":"<pre><code>all = rawer.getAllList( \n    area = \"-180 90,180 90, 180 -90, -180 -90,-180 90\",\n    tags = \"building\",\n    hashtag = \"\",\n    dateFrom = \"\",\n    dateTo = \"\",\n    page = 0\n)\n</code></pre>"},{"location":"API/PythonDB/#get-data-quality-reports-in-csv-or-geojson","title":"Get data quality reports in CSV or GeoJSON","text":"<pre><code>from api import report\nreporter = report.Report(db)\n</code></pre>"},{"location":"API/PythonDB/#get-report-for-geometries","title":"Get report for geometries","text":"<pre><code>results = reporter.getDataQualityGeo(\n    fromDate = \"2022-12-28T00:00:00\", \n    hashtags = [\"hotosm\"],\n    responseType = \"csv\"\n)\n</code></pre>"},{"location":"API/PythonDB/#get-latest-results-for-geometries","title":"Get latest results for geometries","text":"<pre><code>results = reporter.getDataQualityGeoLatest()\n</code></pre> <p>For getting results in CSV format, instead of GeoJSON</p> <pre><code>results = reporter.getDataQualityGeoLatest(\n    responseType = \"csv\"\n)\n</code></pre>"},{"location":"API/PythonDB/#get-report-for-tags","title":"Get report for tags","text":"<pre><code>results = reporter.getDataQualityTag(\n    fromDate = \"2022-12-28T00:00:00\", \n    hashtags = [\"hotosm\"],\n    responseType = \"csv\"\n)\n</code></pre>"},{"location":"API/PythonDB/#get-statistics-for-tags","title":"Get statistics for tags","text":"<pre><code>results = reporter.getDataQualityTagStats(\n    fromDate = \"2022-12-28T00:00:00\", \n    hashtags = [\"hotosm\"],\n    responseType = \"csv\"\n)\n</code></pre>"},{"location":"API/REST/","title":"REST","text":""},{"location":"API/REST/#run-a-restful-api","title":"Run a RESTful API","text":""},{"location":"API/REST/#setup-run","title":"Setup &amp; run","text":""},{"location":"API/REST/#install-requirements","title":"Install requirements","text":"<pre><code>cd python/restapi/ &amp;&amp; pip install -r requirements.txt\n</code></pre>"},{"location":"API/REST/#setup-db-connection","title":"Setup DB connection","text":"<p>Set the database connection string as an environment variable:</p> <pre><code>export UNDERPASS_API_DB=postgresql://localhost/underpass\n</code></pre>"},{"location":"API/REST/#run","title":"Run","text":"<pre><code>uvicorn main:app --reload \n</code></pre>"},{"location":"API/REST/#making-queries","title":"Making queries","text":""},{"location":"API/REST/#raw-data","title":"Raw data","text":""},{"location":"API/REST/#get-polygons","title":"Get polygons","text":"<pre><code>curl http://localhost:8000/raw/polygons -X POST \\\n    -H 'content-type: application/json' \\\n    --data-raw '{\"area\":\"-180 90,180 90, 180 -90, -180 -90,-180 90\", \"tags\": \"building=yes\"}'\n</code></pre>"},{"location":"API/REST/#get-lines","title":"Get lines","text":"<pre><code>curl http://localhost:8000/raw/lines -X POST \\\n    -H 'content-type: application/json' \\\n    --data-raw '{\"area\":\"-180 90,180 90, 180 -90, -180 -90,-180 90\", \"tags\": \"highway\"}'\n</code></pre>"},{"location":"API/REST/#get-nodes","title":"Get nodes","text":"<pre><code>curl http://localhost:8000/raw/nodes -X POST \\\n    -H 'content-type: application/json' \\\n    --data-raw '{\"area\":\"-180 90,180 90, 180 -90, -180 -90,-180 90\", \"tags\": \"amenity\"}'\n</code></pre>"},{"location":"API/REST/#get-all-together-polygons-lines-and-nodes","title":"Get all together (polygons, lines and nodes)","text":"<pre><code>curl http://localhost:8000/raw/all -X POST \\\n    -H 'content-type: application/json' \\\n    --data-raw '{\"area\":\"-180 90,180 90, 180 -90, -180 -90,-180 90\", \"tags\": \"building\"}'\n</code></pre>"},{"location":"API/REST/#get-list-of-polygons","title":"Get list of polygons","text":"<pre><code>curl http://localhost:8000/raw/polygonsList -X POST \\\n    -H 'content-type: application/json' \\\n    --data-raw '{\"area\":\"-180 90,180 90, 180 -90, -180 -90,-180 90\", \"tags\": \"building=yes\"}'\n</code></pre>"},{"location":"API/REST/#get-list-of-lines","title":"Get list of lines","text":"<pre><code>curl http://localhost:8000/raw/linesList -X POST \\\n    -H 'content-type: application/json' \\\n    --data-raw '{\"area\":\"-180 90,180 90, 180 -90, -180 -90,-180 90\", \"tags\": \"highway\"}'\n</code></pre>"},{"location":"API/REST/#get-list-of-nodes","title":"Get list of nodes","text":"<pre><code>curl http://localhost:8000/raw/nodesList -X POST \\\n    -H 'content-type: application/json' \\\n    --data-raw '{\"area\":\"-180 90,180 90, 180 -90, -180 -90,-180 90\", \"tags\": \"amenity\"}'\n</code></pre>"},{"location":"API/REST/#get-list-of-all-together-polygons-lines-and-nodes","title":"Get list of all together (polygons, lines and nodes)","text":"<pre><code>curl http://localhost:8000/raw/allList -X POST \\\n    -H 'content-type: application/json' \\\n    --data-raw '{\"area\":\"-180 90,180 90, 180 -90, -180 -90,-180 90\", \"tags\": \"building\"}'\n</code></pre>"},{"location":"API/REST/#get-data-quality-reports-in-csv-or-geojson","title":"Get data quality reports in CSV or GeoJSON","text":""},{"location":"API/REST/#get-report-for-geometries","title":"Get report for geometries","text":"<pre><code>curl http://localhost:8000/report/dataQualityGeo -X POST \\\n    -H 'content-type: application/json' \\\n    --data-raw '{\"fromDate\":\"2022-12-28T00:00:00\", \"hashtags\": \"hotosm\"}'\n</code></pre> <p>In CSV format instead of GeoJSON:</p> <pre><code>curl http://localhost:8000/report/dataQualityGeo/csv -X POST \\\n    -H 'content-type: application/json' \\\n    --data-raw '{\"fromDate\":\"2022-12-28T00:00:00\", \"hashtags\": \"hotosm\"}'\n</code></pre>"},{"location":"API/REST/#get-report-for-tags","title":"Get report for tags","text":"<pre><code>curl http://localhost:8000/report/dataQualityTags -X POST \\\n    -H 'content-type: application/json' \\\n    --data-raw '{\"fromDate\":\"2022-12-28T00:00:00\", \"hashtags\": \"hotosm\"}'\n</code></pre>"},{"location":"API/REST/#get-statistics-for-tags","title":"Get statistics for tags","text":"<pre><code>curl http://localhost:8000/report/dataQualityTagStats -X POST \\\n    -H 'content-type: application/json' \\\n    --data-raw '{\"fromDate\":\"2022-12-28T00:00:00\", \"hashtags\": \"hotosm\"}'\n</code></pre>"},{"location":"Dev/bootstrapsh/","title":"Using the bootstrap.sh script","text":"<p>This script is prepared for a quick bootstrap of the Underpass database.</p>"},{"location":"Dev/bootstrapsh/#install-requirements","title":"Install requirements","text":"<ul> <li>fiona (pip install fiona)</li> <li>shapely (pip install shapely)</li> <li>osm2pgsql (https://osm2pgsql.org/doc/install.html)</li> <li>psql (https://www.postgresql.org/download/) <pre><code>## Quick start\n\nRun the script passing region, country and DB username as arguments, for example:\n\n```sh\n./bootstrap.sh -r south-america -c ecuador -l yes\n</code></pre></li> </ul> <p>This will:</p> <ol> <li>Delete all entries into the database (WARNING: POTENTIAL LOSS OF DATA)</li> <li>Download raw data from GeoFabrik</li> <li>Run osm2psql for import it</li> <li>Convert the country .poly file to .geojson and install config files</li> <li>Run underpass --bootstrap for bootstrapping the validation table</li> </ol>"},{"location":"Dev/bootstrapsh/#script-options","title":"Script options","text":"<pre><code>-r region (Region for bootstrapping)\n   africa, asia, australia, central-america\n   europe, north-america or south-america\n-c country (Country inside the region)\n-h host (Database host)\n-u user (Database user)\n-p port (Database port)\n-d database (Database name)\n-l yes (Use local files instead of download them)\n</code></pre> <p>Use the <code>-l yes</code> when you have your .pbf and .poly files already downloaded and you don't want to download them again. The script will look for those files using the <code>-r</code> and <code>-c</code> arguments, for example</p> <pre><code>./bootstrap.sh -r south-america -c ecuador -u underpass -l yes\n</code></pre> <p>Will look for these files:</p> <pre><code>ecuador-latests.osm.pbf\necuador.poly\n</code></pre>"},{"location":"Dev/bootstrapsh/#osm-authentication-for-downloading-data","title":"OSM authentication for downloading data","text":"<p>If you want to bootstrap your database with ChangeSet data downloaded from  GeoFabrik, you'll need to be authenticated with an OSM account.</p> <p>A good utility for doing this from command line is in the sendfile_osm_oauth_protector  repository.</p> <pre><code>git clone https://github.com/geofabrik/sendfile_osm_oauth_protector.git\ncd sendfile_osm_oauth_protector\n</code></pre> <p>Edit <code>settings.json</code> with your credentials:</p> <p>```json  {   \"user\": \"\",   \"password\": \"\",   \"osm_host\": \"https://www.openstreetmap.org\",   \"consumer_url\": \"https://osm-internal.download.geofabrik.de/get_cookie\" } <pre><code>And download your file:\n\n```sh\ncurl https://osm-internal.download.geofabrik.de/africa/tanzania-latest-internal.osm.pbf \\\n   --cookie \"$(cat cookie_output_file.txt)\" --output tanzania-latest.osm.pbf\n</code></pre> <p>Then you can move the pbf file to the <code>underpass/utils</code> directory and use the <code>-l yes</code> option.</p>"},{"location":"Dev/changefile/","title":"Replication Files","text":"<p>There are two types of data files related to changes in the map data. These files contain all the changes made during a time interval, every minute, hour, or daily data files are available from the OpenStreetMap planet server. </p> <p>There are two different formats of change data, one for changesets, and the other for the actual changes. Both are needed for filtering changes to produce statistics.</p>"},{"location":"Dev/changefile/#changeset","title":"Changeset","text":"<p>A changeset file contains only the data about the change, and not the actual change itself. It contains the data of the change at the time it's uploaded to OpenStreetMap. Each change has an action; create, delete, modify. Each action then contains the changed objects in the action. As this file contains the data created when uploading it to OpenStreetMap, it's the only way to access the commit hashtags or comments. </p> <p>Hashtags didn\u2019t exist until late 2014, so between 2014 and 2017, hashtags were contained in the comment field. In 2017, the official hashtag tag was added. The first mentions of the hashtags #missingmaps and #hotosm starts 2017-10-20. A typical changefile entry looks like this:</p> <pre><code>    &lt;changeset id=\"12345\" created_at=\"2014-10-10T01:57:09Z\" closed_at=\"2014-10-10T01:57:23Z\" open=\"false\" user=\"foo\" uid=\"54321\" min_lat=\"-2.8042325\" min_lon=\"29.5842812\" max_lat=\"-2.7699398\" max_lon=\"29.6012844\" num_changes=\"569\" comments_count=\"0\"&gt;\n        &lt;tag k=\"source\" v=\"Bing\"/&gt;\n        &lt;tag k=\"comment\" v=\"#hotosm-task-001 #redcross #missingmaps\"/&gt;\n        &lt;tag k=\"created_by\" v=\"JOSM/1.5 (7182 en)\"/&gt;\n    &lt;/changeset&gt;\n</code></pre> <p>The created_by and source fields can be used to generate statistics of editor choices and imagery sources. Underpass uses only the comment and hashtag fields currently as a way to group sets of changes by organization, or mapping campaign.</p>"},{"location":"Dev/changefile/#osmchange","title":"OsmChange","text":"<p>And OsmChange file contains the data of the actual change. It uses the same syntax as an OSM data file plus the addition of one of the three actions. Nodes, ways, and relations can be created, deleted, or modified. Multiple OSM object types can be in the same action. As this data contains the actual change, it is used to filter by tag, or used to do calculations, like the length of roads added.</p> <pre><code>    &lt;modify&gt;\n        &lt;node id=\"12345\" version=\"7\" timestamp=\"2020-10-30T20:40:38Z\" uid=\"111111\" user=\"foo\" changeset=\"93310152\" lat=\"50.9176152\" lon=\"-1.3751891\"/&gt;\n    &lt;/modify&gt;\n    &lt;delete&gt;\n        &lt;node id=\"23456\" version=\"7\" timestamp=\"2020-10-30T20:40:38Z\" uid=\"22222\" user=\"foo\" changeset=\"93310152\" lat=\"50.9176152\" lon=\"-1.3751891\"/&gt;\n    &lt;/delete&gt; \n    &lt;create&gt; \n        &lt;node id=\"34567\" version=\"1\" timestamp=\"2020-10-30T20:15:24Z\" uid=\"3333333\" user=\"bar\" changeset=\"93309184\" lat=\"45.4303763\" lon=\"10.9837526\"/&gt;\\n \n    &lt;/create&gt; \n</code></pre>"},{"location":"Dev/changefile/#state-files","title":"State Files","text":"<p>There are two types of state files, one for changesets and the other for change files. The changeset one is the simplest, only containing the last_run timestamp and a changeset sequence number. The first state file starts 2016-09-07 10:45. An example changeset state file is:</p> <pre><code>\\---\nlast_run: 2020-10-08 16:41:01.863533000 +00:00\nsequence: 4139643\n</code></pre> <p>The state file for change files contains more fields, but the only ones needed for database updates are the timestamp of the data file, and the sequence number. An example change file state file is: </p> <pre><code>#Thu Oct 08 16:38:04 UTC 2020\nsequenceNumber=4229951\ntxnMaxQueried=3081409719\ntxnActiveList=\ntxnReadyList=\ntxnMax=3081409719\ntimestamp=2020-10-08T16\\:38\\:02Z\n</code></pre> <p>As the 3 digit prefix in the state filename matches the data file, this is used to get the right data that matches that state. Since the replication files are fetched via HTTP/HTTPS, there is no timestamp available. To find a specific replication file, a state file close to the timestamp is downloaded. Then the timestamp in the state file is checked, and if it matches, the the 3 digit part of the state file name can be used to find the appropriate replication data file.</p>"},{"location":"Dev/changefile/#finding-the-right-data-file","title":"Finding The Right Data File","text":"<p>It is desirable to be able to start processing changes starting with a specific timestamp. This enables the end user to update databases after a period of downtime. With minute updates, there are many thousands of files. Since it takes about 10 milliseconds to download a state.txt file from planet, scanning for files isn't really practical. Once the proper data file is found, then it's easy to just download the next data file in sequence.</p> <p>Currently, the function <code>PlanetReplicator::findRemotePath</code> is the one responsible for returning the file path from a timestamp. It's using a list of of timestamps and sequence numbers stored in a local file located in <code>config/replicator/confiplanetreplicator.yaml</code> for  calculating the path. This approach must be reviewed in order to have better precision without the need of keeping that file up-to-date.</p>"},{"location":"Dev/chroots/","title":"Chroots","text":"<p>[ THIS DOCUMENT NEEDS REVIEW ]</p>"},{"location":"Dev/chroots/#building-packages-in-a-chroot","title":"Building Packages in a Chroot","text":"<p>To distribute binary packages for a variety of platforms of varying versions, it's common to use a chroot to build in. This is to make sure that a the package binary is linked against the right libraries, which is required so the user doesn't have to build anything from source. Unlike Docker or Virtualbox, a chroot uses the base running kernel on host machine, but provides a different set of runtime libraries and tools.</p>"},{"location":"Dev/chroots/#creating-a-chroot","title":"Creating a Chroot","text":"<p>All platforms that use the .deb packaging format, like Debian and Ubuntu, can use the debbootstrap program to create the chroot. Debootstrap lets you pick the architect to support, <code>--arch amd64</code> builds files for the X86_64 platform. after that the version of the distribution is specified, followed by the directory to install the files in, and finally the URL to download packages from. Once done, change to the installation directory, and type <code>sudo chroot .</code>. You'll now be in a minimal runtime environment. The first thing I do is create a <code>/etc/debian_chroot</code> file containing the name of the distribution. This puts the this value into the command prompt for the shell, which is very useful when you have multiple chroots.</p> <p>As the chroot program requires one to be root, a utility program called schroot can be used to fake root. Once the initial chroot is created, change to the directory containing the downloaded files, and type these shell commands. Some of the programs used to build packages need these, which aren't present in the chroot, as they have to be shared from the host platform.</p> <pre><code>mount -t proc proc /proc\nmount --rbind /sys sys/\nmount --rbind /dev dev/\n</code></pre> <p>Once that initial setup is done, it's time to download more packages. The default config file for a chroot is limited to just the main repository. Not all of the packages Underpass depends on are in main, so add universe to get the rest, and then <code>apt-get update</code>. Your <code>/etc/apt/sources.list</code> file should now look like this:</p> <pre><code>deb http://archive.ubuntu.com/ubuntu focal main universe\n</code></pre> <p>Finally edit  <code>~/.bashrc</code> and add these three lines at the bottom. One specifies the path to a file pkg-config needs, but isn't always distributed, so it's included. The other is for locale, so it stops clutterin the terminal with warnings.</p> <pre><code>export PKG_CONFIG_PATH=/home/rob/underpass/m4\nexport LANG=C\nexport GPG_TTY=\"$(tty)\"\n</code></pre> <p>To create signed packages, you need a GPG key pair setup, so might as well do that now. Type <code>gpg --full-generate-key</code>, and answer the questions. This will only work if you've execute the mount commands as documented above. </p>"},{"location":"Dev/chroots/#for-debian","title":"For Debian","text":"<pre><code>sudo /usr/sbin/debootstrap --arch amd64 bullseye bullseye/ http://deb.debian.org/debian/\n</code></pre> <p>Start by installing packages needed to build Underpass.</p> <pre><code>sudo apt-get -y install \\\n    git gcc g++ pkg-config make debhelper debconf chrpath ccache devscripts gpg \\\n    libgdal-dev libosmium2-dev libpq-dev libgumbo-dev \\\n    libssl-dev libpq5-dev libxml++2.6-dev libboost-all-dev\n</code></pre>"},{"location":"Dev/chroots/#ubuntu-focal-2004-lts","title":"Ubuntu Focal (20.04 LTS)","text":"<pre><code>sudo /usr/sbin/debootstrap --arch amd64 focal focal/ http://archive.ubuntu.com/ubuntu/\n</code></pre> <p>Start by installing packages needed to build Underpass.</p> <pre><code>sudo apt-get -y install \\\n    git make gcc g++ pkg-config gpg debhelper debconf devscripts python3-all \\\n    libgdal-dev libpq-dev libgumbo-dev openmpi-bin \\\n    libxml++2.6-dev ccache libssl-dev libzip-dev libbz2-dev \\\n    libboost-all-dev libosmium2-dev osmium-tool \\\n    doxygen librange-v3-dev libtool-bin libltdl-dev  \n</code></pre>"},{"location":"Dev/chroots/#ubuntu-groovy-2010","title":"Ubuntu Groovy (20.10)","text":"<pre><code>sudo /usr/sbin/debootstrap --arch amd64 groovy groovy/ http://archive.ubuntu.com/ubuntu/\n</code></pre> <p>Install dependencies as above. Note that the version of boost is 1.74</p>"},{"location":"Dev/chroots/#ubuntu-hirsute-2104","title":"Ubuntu Hirsute (21.04)","text":"<pre><code>sudo /usr/sbin/debootstrap --arch amd64 hirsute hirsute/ http://archive.ubuntu.com/ubuntu/\n</code></pre> <p>Install dependencies as above. Note that the version of boost is 1.74</p>"},{"location":"Dev/chroots/#building-libpqxx-packages","title":"Building Libpqxx Packages","text":"<p>The version of libpqxx (6.x) that's included in current (Aug 2021) distribution like Debian Bullseye or Unbuntu Hirsute (21.04) has a bug triggered by libxml++. Since Underpass uses libxml++ and libpqxx, we have to build a package of a newer version (7.x). I added Debian packaging files to a git repository to make this easier, Get that fork here:</p> <pre><code>git clone https://github.com/robsavoye/libpqxx.git\n</code></pre> <p>Once I do that, I run <code>git tag</code>, and the checkout the latest official release branch. There's a configure bug in the libpqxx sources I haven't gotten around to fixing yet, so I edit <code>configure.ac</code>, and comment out this one line like so:</p> <pre><code>dnl AX_CXX_COMPILE_STDCXX_17([noext])\n</code></pre> <p>Then run <code>./autogen.sh</code>, which produces the scripts used for configuring. Now that you have the <code>configure</code> script, configure libpqxx like this:</p> <pre><code>./configure CXX=\"ccache g++\" CXXFLAGS=\"-std=c++17 -g -O2\" --disable-dependency-tracking\n</code></pre> <p>After that, change to the debian directory, and type <code>make deb -i -k</code>. That'll build the deb packages, and at the end, have you GPG sign them. Once built, install the two packages, and you're all set to go build Underpass.</p>"},{"location":"Dev/chroots/#building-underpass-packages","title":"Building Underpass Packages","text":"<p>The Underpass git repository is at:</p> <p>https://github.com/hotosm/underpass.git</p> <p>Once downloaded, change to the source directory and run <code>./autogen.sh</code>. Once the configure files have been built, configure Underpass like this:</p> <pre><code>./configure CXX=\"ccache g++\" CXXFLAGS=\"-std=c++17 -g -O2\" --disable-dependency-tracking\n</code></pre> <p>and then typing <code>make deb -i -k</code> builds the packages, and has you sign them.</p>"},{"location":"Dev/coding/","title":"Underpass Coding Style","text":"<p>This documents the prefered coding style for Underpass. The goal of this coding style is to make code consistent amongst developers, and to write code that is focused on being readable and compact. Because the existing code is written with this goal, using reformatting tools like indent or clang-format is discouraged.</p> <p>The easy way to follow this style is to use cc-mode in Emacs. Spaces will be used instead of TABS, since different systems or users may have TAB set differently. Indents are 4 spaces, instead of 8. </p>"},{"location":"Dev/coding/#documentation","title":"Documentation","text":"<p>All code will use Doxygen style comments. Classes and methods should each have a comment so a short description will be displayed in the output files Doxygen generates. Each file should also have a brief description.</p>"},{"location":"Dev/coding/#line-length","title":"Line Length","text":"<p>For decades lines were limited to 80 charcters or less, which was based on what a CRT could display. With modern monitors this limitation doesn't exist. The Linux kernel has been using 132 character line length, so this project does too.</p>"},{"location":"Dev/coding/#line-breaks","title":"Line breaks","text":"<p>Line breaks are used to break up long lines, with the remainder indented to be clear it's a continuation of the line. Some times a long line is prefered if adding a line break makes the code less readable.</p>"},{"location":"Dev/coding/#brace-location","title":"Brace Location","text":"<p>The opening brace for most code is on the same line as the line of code. Putting an opening brace on a line by itself adds too much unnecessary whitespace. A closing brace is usually on a line by itself.</p> <p>Some examples:</p> <pre><code>namespace replication {\nclass StateFile;\n};\n\nclass RawChangeset {\n  public:\n      void foo(void);\n};\n</code></pre> <p>In C++ a conditional with a single line following the test is legal, but braces should be used to make it clearer.</p> <pre><code>if (i == 20) {\n   // do something\n}\n</code></pre>"},{"location":"Dev/coding/#naming-convention","title":"Naming Convention","text":"<p>Underpass uses Camel Case, which uses capitalization instead of underbars for class definitions, function or C++ methods, and variable. For example RawChange instead of raw_change. Class names capitalize the first letter as well, where the methods in the class start with a lower case letter.</p> <p>Names should be descriptive, and when possible, avoid abbrevations unless they are commonly used.</p>"},{"location":"Dev/dataflow/","title":"Underpass Data Flow","text":"<p>The Overpass data storage contains information from several sources. Underpass has to duplicate this collection of data and aggregate it into a similar structure. The primary data is the map data itself, which contains no history or change data.\u00a0 All the necessary data is available from planet.openstreetmap.org, but is not all in the same format, and is in different places on the planet server. There appear to be no open-source projects that aggregate all this data together other than Overpass.</p> <p>There are two different change file formats. One includes the hashtags and comments from the change. The other is the changed data itself. Both of these data files can be downloaded from the planet server, and can also be updated with minutely changes. There is currently no open source software that I can find to process the changeset file, nor to merge it with the other change data. This is one of the tasks Underpass needs to handle. To support the Missing Maps leaderboard, and OSM Stats. the changes are used to calculate the totals of highways, waterways, and buildings added or edited in that change. A change it the data at uploading to OSM time. Underpass will recreate the history as it processes the change files.</p> <p>For producing statistics, the process initially starts by downloading the large data file of all changes made since 2005. As hashtags didn\u2019t exist until late 2014, data prior to then is ignored. Starting in 2014, hashtags were stored in the comments field of the map data, till the hashtag field was added in late 2017. This data file has some of the fields we want, which is documented here.</p> <p>Briefly, Underpass extracts the hashtag (if there are any) and changeset timestamp. The next task is to download the changed data itself. This is available at planet.openstreetmap.org as well. The osmium program can do this, but does not support importing it into a database. It only works with disk files. The Replicator program will do a similar task, namely download the changes, but will apply them to the statistics database. While Replicator processes the changes, it will also update the statistics database by adding the totals for the new data to the existing amounts. For statistics collection, the core OSM database isn't needed.</p> <p>However the changes do need to be applied to the OSM map database, so when processing exports, or later doing validation, it\u2019s up to date. The entire update process of both databases has to happen within one minute if we want to update that frequently, which is the goal. When updating the map database, any node or way that is changed will be stored in the database in a new table. This will create the history needed for augmented diffs. Only the previous version is stored, any older entries will be deleted from the history database.</p>"},{"location":"Dev/dataflow/#source-data","title":"Source Data","text":"<p>Underpass collects data from multiple sources, primarily the  Change Files from the OpenStreetMap planet server. Changes are availble with different time intervals. The minutely one are the primary data source. There are two sets of minute update, one documents the change and the other is the changed data itself.</p>"},{"location":"Dev/dataflow/#directory-organization","title":"Directory Organization","text":"<p>Almost all directories and files are numerically based. The very top level contains the intervals, and the changeset directories, and looks like this:</p> <pre><code>minute/\nchangeset/\nhour/\nday/\n</code></pre> <p>Under each one of these top level directories are the numerical ones. At the lowest level there are 1000 files in each directory, so the value is always a consistent 3 digit number.</p> <pre><code>...\n002/\n001/\n000/\n</code></pre> <p>Under each on of these sub-directories are the actual data ones</p> <pre><code>999/\n998/\n...\n001/\n000/\n</code></pre> <p>Each one of these directories contains 1000 files, roughly 16 hours of data.</p> <pre><code>replication/002/\nreplication/002/002/\nreplication/002/002/002/\nreplication/002/002/002/\nreplication/002/002/002/002.state.txt\nreplication/002/002/002/002.osm.gz\n</code></pre> <p>The state file contains the ending timestamp of the associated data file. Underpass uses the state.txt file to find the right data. THe timestamps are not a consistent time interval, so it's not really possible to just calculate the right data file. To speed up searching for the right data file based on a timestamp, Underpass downloads them and enters the path to the data file and the timestamp into postgres.</p> <p>This process works forward and backwards. When initializing a database from scratch, Underpass will just bulk download the state files, starting with the most recent ones, and then going backwards in time. It takes about a week to download all the minutely state.txt files, hourly only a few hours.</p> <p>Underpass can also monitor for new data files, so as to keep up to date. If the database is up to date, the threads are put to sleep, and wake up a minute later to see if there are new files. If behind by several minutes, Underpass will bulk download them until caught up.</p> <p>Underpass uses simple threading, so can utilize multiple CPU cores. When a new data file is downloaded, it's uncompressed and parsed. During the parsing process, some data validation is done. If a change fails validation, it's written to a database table so a validator can look into the issue later. If it passes validation, then statistics are calculated, and written to the database.</p>"},{"location":"Dev/dataflow/#output-databases","title":"Output Databases","text":"<p>Underpass currently writes to 1 primary database: the underpass database, which stores all the calculated statistics, data validation results and raw OSM data that also gets updated every minute.</p>"},{"location":"Dev/debugging/","title":"Debugging","text":""},{"location":"Dev/debugging/#development-and-debugging","title":"Development and debugging","text":""},{"location":"Dev/debugging/#build-flags","title":"Build flags","text":"<p>The following flags are suggested when running the configuration for building:</p> <p><code>../configure CXX=\"ccache g++\" CXXFLAGS=\"-std=c++17 -g -O0\" CPPFLAGS=\"-DTIMING_DEBUG\"</code></p>"},{"location":"Dev/debugging/#debugging-with-gdb","title":"Debugging with GDB","text":"<p><code>apt-get install gdb</code></p> <p>Setup an alias for <code>gdb</code> in your bash profile, adding the following line add the bottom of <code>~/.bashrc</code>:</p> <p><code>alias lg='libtool --mode=execute gdb'</code></p> <p>And you'll be able to run the debugger, for example:</p> <p><code>lg src/testsuite/libunderpass.all/yaml-test</code> <code>lg --args ./underpass --bootstrap</code></p>"},{"location":"Dev/debugging/#debugging-in-macos","title":"Debugging in MacOS","text":""},{"location":"Dev/debugging/#with-gdb","title":"With GDB","text":"<p>It's also possible to debug Underpass on MacOS. You should use <code>glibtool</code> instead of <code>libtool</code>.</p> <p><code>brew install gdb</code></p> <p>Note that gdb requires special privileges to access Mac ports. You will need to codesign the binary. For instructions, see: https://sourceware.org/gdb/wiki/PermissionsDarwin</p> <p>Add the alias in your <code>~/.bashrc</code> file:</p> <p><code>alias lg='glibtool --mode=execute gdb'</code></p>"},{"location":"Dev/debugging/#with-lldb","title":"With LLDB","text":"<p>When running Underpass on a MacOS system with an Arm64 architecture, <code>gdb</code> might be not available. You can use <code>lldb</code> instead.</p> <p><code>lldb -- src/testsuite/libunderpass.all/yaml-test</code> <code>lldb -- .libs/underpass --bootstrap</code></p>"},{"location":"Dev/engine/","title":"Underpass","text":"<p>Underpass is a C++ API and utility programs for manipulating OpenStreetMap data at the database and raw data file level. It can download replication files from the OSM planet server and use these files to update a local copy of the OSM database, or analyze the changes to generate statistics. It is designed to be high performance on modest hardware.</p> <p>Currently this is usually done using Overpass, which can be self-hosted or accessed remotely. Overpass has major performance issues though, namely it\u2019s single threaded, and doesn\u2019t use a real database. What Overpass does is support a wide range of querying OSM data, and can handle rather complicated filters. Much of that functionality is not needed for supporting statistics collection and simple validation.</p> <p>Underpass is designed to function as a replacement for a subset of Overpass\u2019s functionality. It is focused on analyzing the change data every minute, and generating statistics or doing validation of the metadata. It is designed to be able to process large files by streaming the data.</p> <p>Rather than using disk based tempfiles, it has a self-hosted database centric design for better performance. It uses Postgresql with the Postgis extension, both commonly used for core OSM infrastructure. One big advantage of using postgres is it supports utilizing multi-core processors for faster SQL queries. It can optionally use GPU support for faster geospatial queries. </p> <p>The other primary design goal of Underpass is to be maintainable for the long term. It uses common open source infrastructure to make it accessible to community developers. The primary dependencies, which are used by multiple other core OSM projects are these:</p> <ul> <li>Uses the GNU autotools for multi-platform support</li> <li>Uses SWIG for bindings to multiple languages</li> <li>Uses Boost for additional C++ libraries</li> <li>Uses GDAL for reading Geospatial files</li> <li>Uses PQXX for accessing Postgres</li> <li>Uses Libxml++ for parsing XML files</li> <li>Uses Doxygen for producing code documentation</li> </ul> <p>Ideally Underpass can be used by other projects needing to do similar tasks without Overpass. It should be able to support collecting more statistics than are currently used. Since much of Underpass deals with the low level data flow of OpenStreetMap, long-term it can be used to support future projects as a common infrastructure for database oriented mapping tasks.</p> <p>A critical future project is conflation and validation of existing data and mapathons. Underpass will support the database management tasks those projects will need. The same database can also be used for data exports, and Underpass can be used to keep that data up to date. What the Overpass data store contains is history information, change information, and the actual OSM data. Underpass can access this same information by processing the change data itself.</p>"},{"location":"Dev/engine/#dependencies","title":"Dependencies","text":"<p>Underpass requires a modern C++ compiler that supports the 2011 C++ standard. The 2011 standard simplified the syntax by adding the auto keyword, and thread support became part of the standard C++ library. Underpass is a heavy user of of the boost libraries, and also requires reasonably up to date C++ libraries for other dependencies. Underpass also uses the ranges-v3 library. This will be in the 2020 C++ standard, but hasn't been released yet.</p> <p>Fedora 33, Debian Buster, and Ubuntu Groovy contain a package for librange-v3, which is the code base for what will be in C++ 2020. Both Debian Buster and Ubuntu Groovy ship libpqxx 6.x, which has a bug which has been fixed in libpqxx 7.x. Fedora 33 ships the newer version.</p>"},{"location":"Dev/install-with-docker/","title":"Install with docker","text":""},{"location":"Dev/install-with-docker/#docker-installation","title":"Docker installation","text":"<p>This is the easiest way to get Underpass up and running:</p> <pre><code>docker-compose up -d\n</code></pre>"},{"location":"Dev/install-with-docker/#setup-and-run-demo","title":"Setup and run demo","text":""},{"location":"Dev/install-with-docker/#bootstrap-the-database-for-a-country","title":"Bootstrap the database for a country","text":"<p>Requirements:</p> <ul> <li>fiona (pip install fiona)</li> <li>shapely (pip install shapely)</li> <li>osm2pgsql (https://osm2pgsql.org/doc/install.html)</li> <li>psql (https://www.postgresql.org/download/)</li> </ul> <p>Run this utility script for bootstrap the database with data for a country:</p> <p><pre><code>cd setup &amp;&amp; ./bootstrap.sh -r asia -c nepal -p 5439\n</code></pre> Regions (-r) are:</p> <ul> <li>africa</li> <li>asia</li> <li>australia-oceania</li> <li>central-america</li> <li>europe</li> <li>north-america</li> <li>south-america</li> </ul> <p>Countries (-c) is the name of the country inside the region.</p> <p>Data is downloaded from GeoFabrik, if you are not sure of what name you need to use, please check there.</p>"},{"location":"Dev/install-with-docker/#configure-the-ui","title":"Configure the UI","text":"<p>You can find the UI's playground here: <code>http://localhost:8080</code></p> <p>Edit this file with the center coordinates for the map:</p> <p><code>js/src/fixtures/center.js</code></p> <p>And copy it to the container:</p> <pre><code>docker cp js/src/fixtures/center.js underpass_ui:/code/src/fixtures/center.js\n</code></pre> <p>Refresh the page and you'll see your map centered in the new coordinates, displaying raw data and validation results on top of the OSM map.</p>"},{"location":"Dev/install-with-docker/#keep-the-database-up-to-date-minutely","title":"Keep the database up-to-date minutely","text":"<p>Run this command for start processing data from 2 days ago:</p> <pre><code>docker exec -d underpass underpass -t $(date +%Y-%m-%dT%H:%M:%S -d \"2 days ago\")\n</code></pre> <p>From MacOS, the date command is different:</p> <pre><code>docker exec -d underpass underpass -t $(date -v -2d +%Y-%m-%dT%H:%M:%S)\n</code></pre> <p>For running underpass as a daemon, use the <code>-d</code> option:</p> <pre><code>docker exec -d underpass underpass -t $(date -v -2d +%Y-%m-%dT%H:%M:%S)\n</code></pre>"},{"location":"Dev/install-with-docker/#stop-and-start-underpass","title":"Stop and start underpass","text":"<p>If you want to stop the process, you can kill the container:</p> <pre><code>docker kill underpass\n</code></pre> <p>Then, you'll have to run <code>docker-compose up -d</code> before starting the process again.</p>"},{"location":"Dev/install/","title":"Underpass installation","text":"<p>Tested in operating system Ubuntu 22.10</p>"},{"location":"Dev/install/#install-binary-dependencies","title":"Install binary dependencies","text":"<pre><code>sudo apt-get update \\\n    &amp;&amp; apt-get install -y software-properties-common \\\n    &amp;&amp; apt-get update &amp;&amp; apt-get install -y \\\n        libboost-dev \\\n        autotools-dev \\\n        swig \\\n        pkg-config \\\n        gcc \\\n        build-essential \\\n        ccache \\\n        libboost-all-dev \\\n        dejagnu \\\n        libjemalloc-dev \\\n        libxml++2.6-dev \\\n        doxygen \\\n        libgdal-dev \\\n        libosmium2-dev \\\n        libpqxx-dev \\\n        postgresql \\\n        libgumbo-dev \\\n        librange-v3-dev\n</code></pre>"},{"location":"Dev/install/#build-underpass","title":"Build Underpass","text":"<pre><code>$ ./autogen.sh &amp;&amp; \\\n  mkdir build &amp;&amp; cd build &amp;&amp; \\ \n  ../configure &amp;&amp; make -j$(nproc) &amp;&amp; sudo make install\n</code></pre>"},{"location":"Dev/osmstats/","title":"OSM Stats","text":"<p>The Red Cross maintains statistics of changes, so it\u2019s possible to track the mapping progress of individual users, teams, and mapping campaigns. Underpass writes directly to the postgres database used for OSM Stats. Underpass has the ability to recreate this database and populate it with data from the raw data files. Underpass uses replication change files to update these tables. The database schema contains a number of tables which are documented here:</p> <p> </p>"},{"location":"Dev/osmstats/#osm-stats-tables","title":"OSM Stats Tables","text":"<p>The OSM schema contains a number of tables, which are designed to support the web front end. the raw_ tables are mostly static, and are simply a matching of an ID to a name for display. There are other smaller tables for the same purpose, usedc to group queries together for the front-end.</p> Keyword Description Augmented_diff_status Contains the changeset ID of the augmented diff, and the timestamp of the last update badges Contains the the badge ID, the category, the badge name, and the experience level badges_users Contains the user ID, the badge category, the badge name, and the user experience level changesets_status Contains the changeset ID and the timestamp for the update raw_changesets Contains all the data of the changeset raw_changesets_countries Contains the changeset ID and an index into the raw_changesets table raw_changesets_hashtags Contains the changeset ID and the an index into the raw_hashtag table raw_countries Contains an index number, and the country or state name plus a display abbreviation raw_hashtags Contains an index number and hashtag used raw_users Contains only the user ID and name spatial_ref_sys Geospatial data used by Postgis badge_updater_status <p> </p>"},{"location":"Dev/osmstats/#raw_changesets-table","title":"raw_changesets Table","text":"<p>The main table is raw_changesets, which contains extracted data from the two files is then processed to create the various statistics. The counters use the existing data, and only add to this value based on what is in the change file, as this requires much less processing time. This is the primary table Underpass updates the data in.</p> Keyword Description id This is the ID of the changeset, and comes from the changeset file road_km_added This value is updated by counting the length of roads added or deleted by the user in the change file road_km_modified This value is updated by counting the length of existing roads modified by the user in the change file waterway_km_added This value is updated by counting the length of waterways added or deleted by the user in the change file waterway_km_modified This value is updated by counting the length of existing waterways modified by the user in the change file roads_added This value is updated by counting the roads added or deleted by the user in the change file roads_modified This value is updated by counting the existing roads modified by the user in the change file waterways_added This value is updated by counting the roads added or deleted by the user in the change file waterways_modified This value is updated by counting the existing waterways modified by the user in the change file buildings_added This value is updated by counting the buildings added by the user in the change file buildings_modified This value is updated by counting the existing buildings modified by the user in the change file pois_added This value is updated by counting the POIs added by the user in the change file pois_modified This value is updated by counting the existing POIs modified by the user in the change file editor The editor used, and comes from the changeset uid The user ID, comes from the changeset created_at The timestamp this changeset was created, comes from the changeset closed_at The timestamp this changeset was closed, comes from the changeset verified Whether this data has been validated updated_at The timestamp this change was applied to the database augmented_diffs <p> </p>"},{"location":"Dev/osmstats/#raw_users-table","title":"raw_users Table","text":"Keyword Description id OSM user ID name OSM username"},{"location":"Dev/osmstats/#raw_hashtags-table","title":"raw_hashtags Table","text":"Keyword Description id hashtag ID, internal use only hashtag OSM username"},{"location":"Dev/osmstats/#raw_countries-table","title":"raw_countries Table","text":"Keyword Description id country ID, internal use only name Country full name code The 3 letter ISO abbreviation"},{"location":"Dev/osmstats/#raw_changesets_countries-table","title":"raw_changesets_countries Table","text":"Keyword Description changeset_id The changeset ID country_id The country ID"},{"location":"Dev/osmstats/#raw_changesets_hashtags-table","title":"raw_changesets_hashtags Table","text":"Keyword Description changeset_id The changeset ID hashtag_id The hashtag ID"},{"location":"Dev/osmstats/#changesets_status-table","title":"changesets_status Table","text":"Keyword Description id The changeset ID updated_at Timestamp of the update"},{"location":"Dev/osmstats/#badge-table","title":"badge Table","text":"Keyword Description id The badge ID category The badge catagory name The badge name level The badge level"},{"location":"Dev/osmstats/#badges_users-table","title":"badges_users Table","text":"Keyword Description uid The OSM user ID badge_id The badge ID updated_at The timestamp of the user receiving this badge"},{"location":"Dev/osmstats/#augmented_diff_status-table","title":"augmented_diff_status Table","text":"Keyword Description id The change ID updated_at The timestamp when this change was applied"},{"location":"Dev/replication/","title":"Replication","text":"<p>Replication is the process of appyling a set changes to data. There are two types of replication files available for OpenStreetMap data, changesets and changefiles from their planet server. The level directory contains several subdirectories. All the subdirectory structures are the same.</p> <p>Each of the directories contains a 3 digit name. A new directory is created whenever there are 1 million data files created. Each of these top level's subdirectories contains 1000 subdirectories, also named with a 3 digit value. Under those subdirectories, there are 1000 entries. The time interval between map updates is not consistent, so a state.txt file is used. This file contains the timestamp of the accompanying data file. The data file is compressed, and contains all the changed data.</p> <p>For the change files, the directory structure looks like this:</p> <pre><code>minute/001/001/001/001.state.txt\nminute/001/001/001/001.osc.gz.\nminute/001/001/001/002.state.txt\nminute/001/001/001/002.osc.gz.\netc...\n</code></pre> <p>Changesets are slightly different, as they contain no map data, just data on the changes made when uploaded. </p> <pre><code>changesets/001/001/001.osm.gz\nchangesets/001/001/001.state.txt\netc...\n</code></pre> <p>Since the data file uses the same 3 digit prefix as the state file, it's easy to get the right data for the timestamp. Because the size of the data in the changefiles varies, plus CPU load, network latency, etc... the time interval varies, so can't be calculated accurately. It is possible to get a rough idea. Underpass makes the best guess it can, and downloads a state.txt file that is close to the desired timestamp. Using that initial state.txt file it's possible to increment or decrement the prefix till the proper timestamp is found. Then the prefix is used to download the data file for processing.</p>"},{"location":"Dev/replicator/","title":"Underpass command line utility","text":"<p>underpass is a command line utility for updating databases from changesets and change files, and runs as a system daemon. Both change files and changesets need to be applied to the database to stay in sync. Change data can be downloaded from planet.openstreetmap.org every minute, and has to be applied to the databases. Data is processed as a stream, so a large disk for temporary files or much memory isn\u2019t needed.</p> <p>The primary mode is to monitor the upstream change data for new files, and to apply them. If the timestamp specifed is in the past, the change data files are downloaded continuously until caught up with the current time.</p> <p>Each data source uses a different thread for multi-core support and better performance. As files are downloaded from the different sources, they are analyzed, and applied to the appropriate database. Most of the work is done by the Underpass library. underpass is the utility program that a uses the library. </p> <pre><code>underpass -h\n-h [ --help ]         display help\n-s [ --server arg]    database server (defaults to localhost)\n-m [ --monitor]       Start monitoring planet\n-t [ --timestamp arg] Starting timestamp\n-i [ --import ] arg   Initialize pgsnapshot database with datafile\n</code></pre> <p>By default, underpass uses the last date entered into the OSM Stats datavase as the starting point. A different timestamp can also be specified using --timestamp. All data files downloaded can optionally be cached to disk, so furthur processing can use those and save on network time.</p>"},{"location":"Dev/statistics/","title":"Statistics Calculations","text":"<p>Underpass process two input streams for data, but only one is used for the initial statistics calculations. ChangeSet are used to populate some of the columns in the database, but aren't used during statistics calculations by the backend. Only the OsmChange file is used, as it contains the the data that is changed.</p> <p>The OsmChange data file contains 3 categories of data, what was created, modified, or deleted. Currently only changed and modified data are used for statistics calculations.</p> <p>All of the changed data is parsed into a data structure that can be passed between classes. This data structure contains all the data as well as the associated action, create or modify. The parsed data is then passed to the OsmChangeFile::collectStat() method to do the calculations. While currently part of the core code, in the future this will be a plugin, allowing others to create different statistics calculations without modifying the code.</p>"},{"location":"Dev/statistics/#priority-boundary","title":"Priority boundary","text":"<p>Currently, changes to be proccessed are filtered by ChangeSetFile::areaFilter() and OsmChangeFile::areaFilter(), using a boundary polygon. In some cases is not possible to say if a OsmChange is inside the priority boundary. To disable the filtering in the replication process, you can add the argument <code>--osmnoboundary</code> for OsmChanges or <code>--oscnoboundary</code> for Changesets.</p>"},{"location":"Dev/statistics/#what-is-collected","title":"What Is Collected","text":"<p>The original statistics counted buildings, waterways, and POIs. The new statistics break this down into two categories, accumulates statistics for things like buildings, as well as the more detailed representation, like what type of building it is. The list of values is configurable, as it uses a YAML file.</p> <p>OpenStreetMap features support a keyword and value pair. The keywords are loosely defined, and over time some have changed and been improved. Often new mappers get confused, so may use keywords and values in an inconsistent manner. Also over time the definitions of some tags has changed, or been extended.</p> <p>For example, let's look at schools. There is a variety of ways school buildings are tagged. Sometimes it's building=school, with school as the value. Sometimes school is the keyword, and the value is the type of school. In this case, the type of school is accumulated, as well as a generic school count. Also if building=school isn't used, then every school also increments the count of buildings.</p> <p>Each category of data has the total accumulated value, as well as the more detailed breakdown. For example, it's possible to extract statistics for only hospitals, which is a subset of all the buildings. As keywords and values can be in different feature categories, some are checked for in multiple ways. Some keywords and values may be spelled differently than the default, so variations are also looked for to be complete.</p> <p>To find all the common tags, several continents worth of data was analyzed to find the most common patterns for the features we want to collect statistics for. Mixed with inconsistent tagging schemes is random capitalization, misspellings, and international spellings. The attempt is made to catch all reasonable variations. TagInfo was used to find totals of some variations, with weird tagging that was not very common gets ignored to avoid performance impacts, and data bloat.</p>"},{"location":"Dev/statistics/#building-types","title":"Building Types","text":"<p>Most buildings added by remote tracing of satellite imagery lack any metadata tags beyond building=yes. When local mappers import more detailed data, or update the existing metadata, those values get added. This is a common set of building values.</p> <ul> <li>yes</li> <li>house</li> <li>residential</li> <li>commercial</li> <li>retail</li> <li>commercial;residential</li> <li>apartments</li> <li>kitchen</li> <li>roof</li> <li>construction</li> <li>school</li> <li>clinic</li> <li>hospital</li> <li>office</li> <li>public</li> <li>church</li> <li>mosque</li> <li>temple</li> <li>service</li> <li>warehouse</li> <li>industrial</li> <li>kiosk</li> <li>abandoned</li> <li>cabin</li> <li>bungalow</li> <li>hotel</li> <li>farm</li> <li>hut</li> <li>train_station</li> <li>house_boat</li> <li>barn</li> <li>historic</li> <li>latrine</li> <li>latrines</li> <li>toilet</li> <li>toilets</li> </ul>"},{"location":"Dev/statistics/#amenity-types","title":"Amenity Types","text":"<p>Most amenities are added by local mappers or through a data import. Not all amenities are buildings, but for our use case that's all that is analyzed. These are the common values for the amenity keyword.</p> <ul> <li>hospital</li> <li>school</li> <li>clinic</li> <li>kindergarten</li> <li>drinking_water</li> <li>health_facility</li> <li>health_center</li> <li>healthcare</li> </ul>"},{"location":"Dev/statistics/#places-types","title":"Places Types","text":"<p>Places contain multiple features, and are mostly used to determine local metadata improvements. </p> <ul> <li>village</li> <li>hamlet</li> <li>neighborhood</li> <li>city</li> <li>town</li> </ul>"},{"location":"Dev/statistics/#highway-types","title":"Highway Types","text":"<p>Highways traced from satellite imagery often lack metadata beyond the functional type. For example, a highway that connects two villages is easy to determine. An accumulated value for the total of highways, and the total length in kilometers is store as an aggregate. More detailed statistics are also kept allowing more detail when needed.</p> <ul> <li>trunk</li> <li>tertiary</li> <li>secondary</li> <li>unclassified</li> <li>track</li> <li>residential</li> <li>path</li> <li>service</li> <li>bridge</li> </ul>"},{"location":"Dev/statistics/#school-types","title":"School Types","text":"<p>When school is a keyword, there are several values for the type of school. An aggregate total of schools can be calculated, as well as detail for the type of school.</p> <ul> <li>primary</li> <li>secondary</li> <li>kindergarten</li> </ul>"},{"location":"Dev/statistics/#calculation-data-flow","title":"Calculation Data flow","text":"<p>The data is processed by Underpass. Underpass downloads the changeset and the OsmChange files from the OpenStreetMap planet server every minute. Downloaded files are also cached on disk, so it's also possible to process data without a network connection. Once the data is parsed from the respective data formats, it gets passed to the OsmChangeFile::collectStat() method. That method loops through the data structure containing the changes to the map data. Within that method, it calls ChangeSetFile::scanTags(),     which does all the real work. The scanTags() method uses StatsConfigSearch::search() to     search the lists of keywords and values configured at the stats configuration file. ScanTags() returns an array of statistics for the desired features. That array is then converted by collectStats() into the statistics data structure, and control returns to the processing thread.</p> <p>The processing thread then passes the statistics data to osmstats::applyChange(), to insert them into the database.</p>"},{"location":"Dev/statistics/#changesets-table","title":"changesets table","text":"<p>This is the primary table used to contain the data for each changeset. All of the data is stored in a table for better query performance on large data sets. It also limits needing SQL sub queries or a JOIN between tables, reducing complexity.</p> <p>An hstore is used to store the statistics instead of having a separate table for each feature to allow for more flexibility. An hstore is a key &amp; value pair, and multiple data items can be stored in a single column, indexed by the key. This allows for more features to be added by the backend and the frontend, without having modify the database schema.</p> <p>An example query to count the total number of buildings added by the user 4321 for a Tasking Manager project 1234 would be this:</p> <p>SELECT SUM(CAST(added::hstore-&gt;'building' AS DOUBLE precision)) FROM changesets WHERE 'hotosm-project-1234' = ANY(hashtags) AND uid=4321;</p> <p>The source is the satellite imagery used for remote mapping.</p> <p> </p> Keyword Description id The ID of this changeset editor The editor used for this changeset uid The OSM User ID of the mapper created_at The timestamp when this changes was uploaded closed_at The timestamp when this uploaded change completed processing updated_at The timestamp when this last had data updated added An hstore array of the added map features modified An hstore array of the modified map features deleted An hstore array of the deleted map features hashtags An array of the hashtags used for this changeset source The imagery source used for this changeset bbox The bounding box of this changeset"},{"location":"Dev/stats-test/","title":"Tests for statistics collection","text":"<p>As statistics are a key feature for Underpass, there are several ways to test results and configurations.</p>"},{"location":"Dev/stats-test/#default","title":"Default","text":"<p>The default test will use two files:</p> <ul> <li>OsmChange (testsuite/testdata/test_stats.osc)</li> <li>Stats results validation (testsuite/testdata/test_stats.yaml)</li> </ul> <p><code>./stats-test</code></p>"},{"location":"Dev/stats-test/#more-testing","title":"More testing","text":"<p>You can run other existing tests or create your own custom ones.</p>"},{"location":"Dev/stats-test/#stats-extraction-from-a-single-file","title":"Stats extraction from a single file","text":"<p>Extract stats from file and print the JSON result:</p> <p><code>./src/testsuite/libunderpass.all/stats-test -f stats/107235440.xml</code></p> <pre><code>{\n    \"added\":[\n        {\"highway\":8},\n        {\"highway_km\":64917794}\n    ],\n    \"modified\":[\n        {\"highway\":8}\n    ]\n}\n</code></pre>"},{"location":"Dev/stats-test/#stats-validation-from-files","title":"Stats validation from files","text":"<p>If you want to assert the results agains a YAML file, add second <code>-f</code> argument: </p> <p><code>./src/testsuite/libunderpass.all/stats-test -f /stats/107235440.xml -f /stats/107235440.yaml</code></p> <pre><code>    PASSED: Calculating added highway\n    PASSED: Calculating modified highway\n</code></pre> <p>The YAML file contain the expected results:</p> <pre><code>- modified_highway:\n  - 8\n- added_highway:\n  - 8\n</code></pre>"},{"location":"Dev/stats-test/#collect-stats","title":"Collect stats","text":"<p>Run a replicator process from timestamp, incrementing the path and collecting stats from OsmChange files. This is useful for doing a comparison with other sources (ex: Insights DB).</p> <p><code>./src/testsuite/libunderpass.all/stats-test -m collect-stats -t 2021-05-11T17:00:00 -i 10 &gt; result.json</code></p>"},{"location":"Dev/stats-test/#stats-configuration-file","title":"Stats configuration file","text":"<p>Stats collection can be customized using a YAML configuration file.</p> <p>In the following example, we have an OsmChange file with several nodes created, using building and emergency keys for tags.</p> <p>There are two different configurations for collecting stats from that file. On <code>statsconfig2.yaml</code> , building, fire_station, hospital and police are different categories of stats.</p> <p><code>./src/testsuite/libunderpass.all/stats-test -f stats/test_statsconfig.osc \\     --statsconfigfile /src/testsuite/testdata/stats/statsconfig2.yaml</code></p> <p>Running the command above, you'll see this results:</p> <pre><code>{\n    \"added\":[\n        {\"building\":1},\n        {\"fire_station\":2},\n        {\"hospital\":2},\n        {\"police\":1}\n    ],\n    \"modified\": []\n}\n</code></pre> <p>You can assert this results adding another <code>-f</code> argument:</p> <p><code>./src/testsuite/libunderpass.all/stats-test -f stats/test_statsconfig.osc \\     --statsconfigfile /src/testsuite/testdata/stats/statsconfig2.yaml \\     -f stats/test_statsconfig2.yaml</code></p> <p>On the other configuration file (<code>statsconfig3.yaml</code>) there are two categories for stats: building and humanitarian_building.</p> <p>If you get stats from that file:</p> <p><code>./src/testsuite/libunderpass.all/stats-test -f stats/test_statsconfig.osc \\     --statsconfigfile /src/testsuite/testdata/stats/statsconfig2.yaml</code></p> <p>You'll see a different result:</p> <pre><code>{\n    \"added\":[\n        {\"building\":2},\n        {\"humanitarian_building\":4}\n    ],\n    \"modified\": []\n}\n</code></pre> <p>Again, you can assert the results:</p> <p><code>./src/testsuite/libunderpass.all/stats-test -f stats/test_statsconfig.osc \\     --statsconfigfile /src/testsuite/testdata/stats/statsconfig2.yaml \\     -f stats/test_statsconfig2.yaml</code></p>"},{"location":"Dev/utility/","title":"Utility Programs","text":""},{"location":"Dev/utility/#datadbsetupdbsh","title":"data/db/setupdb.sh","text":"<p>This is a simple shell script that creates all the databases Underpass uses. It can also be used to initialize the two internal databases Underpass uses, one for the *.state.txt files that contain the timestamp of the data files, and the other is for the boundaries of countries used to determine which country a change was made in for statistics collection. These are used to bootstrap a new Underpass installation.</p>"},{"location":"Dev/utility/#setupbootstrapsh","title":"setup/bootstrap.sh","text":"<ul> <li>Using the bootstrap.sh script</li> </ul>"},{"location":"Dev/utility/#utilsfeatures2yaml","title":"utils/features2yaml","text":"<p>Get tags from the Map Features OSM wiki and output YAML</p> <p>Use this script for output a YAML data model that will be used in  the configuration files inside /validation directory.</p>"},{"location":"Dev/utility/#usage","title":"Usage","text":"<pre><code>python features2yaml.py --category buildings &gt; buildings.yaml\npython features2yaml.py --key building:material &gt; building:material.yaml\npython features2yaml.py --url https://wiki.openstreetmap.org/wiki/Key:landuse\npython features2yaml.py --f ../../place.html\n</code></pre> <p>You may want to add more configuration parameters to the YAML file later, like geometry angles or required tags.</p> <p>Dependencies:</p> <ul> <li>pip install beautifulsoup4</li> <li>pip install requests</li> </ul>"},{"location":"Dev/utility/#utilsxlstoyaml","title":"utils/xlstoyaml","text":"<p>Convert data models from XLSForms to YAML</p> <p>Use this script for output a YAML data model that will be used in the configuration files inside /validation directory.</p>"},{"location":"Dev/utility/#usage_1","title":"Usage","text":"<pre><code>python xls2yaml.py --category buildings &gt; buildings.yaml\n</code></pre> <p>You may want to add more configuration parameters to the YAML file later.</p>"},{"location":"Dev/utility/#utilspoly2geojson","title":"utils/poly2geojson","text":"<p>Converts .poly to .geojson </p>"},{"location":"Dev/utility/#usage_2","title":"Usage","text":"<pre><code>python poly2geojson file.poly\n</code></pre>"},{"location":"Dev/utility/#utilsclean-osmchangessql","title":"utils/clean-osmchanges.sql","text":"<p>You may want to run this query if you're not processing raw data and you want to just produce statistics. </p> <p>This query deletes entries created from OsmChanges, closed 1 day back or before , that has no corresponding Changeset.</p> <p>We run this query every 24 hours for cleaning the database when running the replicator process with areaFilter disabled  for osmchanges (--osmnoboundary).</p>"},{"location":"Dev/validation/","title":"Validating Data","text":"<p>It is important to make sure any data being added to OpenStreetMap is of good quality. While there are number of other tools available to do this, Underpass data validation is oriented towards real-time analysis. Most all of the other tools have a time delay of hours or days. As validation can be computationally intensive, there are two levels of validation. Since Underpass is processing change files every minute, the data validation must also fit within that time frame. Also the data available in the minute updates is not often complete, so some types of validation are impossible.</p> <p>The first level of validation is simple, checking for the proper values for tags. For buildings, the geometry can be checked to make sure it's got 90 degree corners or is round. Also checks for overlapping with other buildings in the same changeset is also done.</p> <p>The second level can't be done on huge datasets, but works well for smaller datasets, for example a Tasking Manager project or a county. This other level requires access to OSM data. Since querying a large database may not be possible within the minute timeframe, this is only enabled for datasets country sized or smaller. Using OSM data, it's possible to identify duplicate buildings and highways.</p> <p>The first level of data validation is applied to all changes, since it can be completed within a minute. The second level of validation is focused on Tasking Manager projects. The primary goal of this secondary validation is to catch most mapping errors right away,</p>"},{"location":"Dev/validation/#the-database","title":"The Database","text":"<p>All mapping errors are written to a table called validation in the Galaxy database. This table contains the OSM ID of the feature, the changeset ID that contains this change, and the user ID of the mapper making the change. The status column is an array of the issues. Those issues include bad building geometry, bad tag value, orphan node, duplicate buildings, overlapping buildings, and incomplete tagging. The location of the feature is a single node, which can be used for spatial filtering.</p> <p>In addition, there are currently two debugging columns in the database. One is the timestamp of when the validation is performed, and the other is the calculated angle for buildings that are determined to have bad geometry, which is used to debug the calculation.</p>"},{"location":"Dev/validation/#tag-validation","title":"Tag Validation","text":"<p>Organized mapping campaigns, especially those done on the ground using (ODK)[https://opendatakit.org/] based mobile tools, have a defined set of metadata tags. Often more detailed tags are added after remote mapping by local mappers, as it's impossible to determine some features from the satellite imagery, like building material.</p> <p>HOT currently has a tool called (MapCampaigner)[https://campaigns-staging.hotosm.org/], that is used to validate tag completeness. Required tags are defined in a (YAML)[https://www.yaml.org] config file. For example, to completely map a building, the material of the walls of the building, the material used for a roof, and whether it has walls. If the building is an amenity, it should have a name and the type of amenity. The existing list of tag values were based on what MapCampaigner uses, and then extended by the Data Quality team at HOT. The current values are listed in the config files:</p> <p>amenities,  buildings,  highways,  solid waste,  landuse</p> <p>By default, tag completeness is not enabled, as any data added by remote mapping will always be missing tags, or have different permissible values. Instead tag completeness can be used to monitor a ground mapping campaign.</p>"},{"location":"Dev/validation/#bad-geometry","title":"Bad Geometry","text":"<p>Often buildings are added that don't have square corners. This can be eliminated by using a plugin for some map editors like JOSM to force all new buildings to have the correct geometry. There are other plugins that allow the mapper or validator to correct the geometry. Flagging this error while the mapper is still active allows for educating the mapper about this issue so they can improve their quality, which might be as simple as having them use a building plugin.</p> <p>To determine the building correctness, a corner is chosen, and the angle between the two lines is calculated, A threshold is then applied so any building that looks rectangular to the user is considered acceptable. That threshold value is in the YAML file for building validation so it's easy to adjust.</p> <p>Since Underpass is using minute change files, sometimes it's difficult to validate buildings that have been edited, as not all the nodes in the building polygon are in the change file. Most remote mapping only adds buildings, any editing is done later by a human validator.</p>"},{"location":"Dev/validation/#highway-validation","title":"Highway Validation","text":"<p>Validating highways starts the correct tag value. More than validating the values for the highway tag, this also will check the values of important tags, like surface, smoothness, tracktype, and sac_scale. The important validation is making sure any new highway actually connects to the highway network. Otherwise navigation doesn't work.</p>"},{"location":"Replication/Advanced/","title":"Advanced","text":""},{"location":"Replication/Advanced/#replication-advanced-options","title":"Replication advanced options","text":"<p>You might run the <code>underpass</code> command with the following options:</p> <pre><code>  -h [ --help ]            display help\n  -s [ --server ] arg      Database server for replicator output (defaults to \n                           localhost/underpass) can be a hostname or a full \n                           connection string USER:PASSSWORD@HOST/DATABASENAME\n  -p [ --planet ] arg      Replication server (defaults to planet.maps.mail.ru)\n  -u [ --url ] arg         Starting URL path (ex. 000/075/000), takes \n                           precedence over 'timestamp' option\n  --changeseturl arg       Starting URL path for ChangeSet (ex. 000/075/000), \n                           takes precedence over 'timestamp' option\n  -t [ --timestamp ] arg   Starting timestamp (can be used 2 times to set a \n                           range)\n  -b [ --boundary ] arg    Boundary polygon file name\n  --osmnoboundary          Disable boundary polygon for OsmChanges\n  --oscnoboundary          Disable boundary polygon for Changesets\n  --datadir arg            Base directory for cached files (with ending slash)\n  -v [ --verbose ]         Enable verbosity\n  -d [ --debug ]           Enable debug messages for developers\n  -l [ --logstdout ]       Enable logging to stdout, default is log to \n                           underpass.log\n  -c [ --concurrency ] arg Concurrency\n  --changesets             Changesets only\n  --osmchanges             OsmChanges only\n  --disable-stats          Disable statistics\n  --disable-validation     Disable validation\n  --disable-raw            Disable raw OSM data\n  --bootstrap              Bootstrap data tables\n</code></pre>"},{"location":"Replication/Run/","title":"Run","text":""},{"location":"Replication/Run/#keep-the-database-up-to-date-minutely","title":"Keep the database up-to-date minutely","text":"<p>Run this command for start processing data from 2 days ago</p> <p><code>underpass -t $(date +%Y-%m-%dT%H:%M:%S -d \"2 days ago\")</code></p> <p>On MacOS, the date command is different</p> <p><code>underpass -t $(date -v -2d +%Y-%m-%dT%H:%M:%S)</code></p> <p>Using Docker, add <code>docker exec -d underpass</code></p> <p><code>docker exec -d underpass underpass -t $(date -v -2d +%Y-%m-%dT%H:%M:%S)</code></p> <p>For running underpass as a daemon, use the -d option:</p> <p><code>docker exec -d underpass underpass -t $(date -v -2d +%Y-%m-%dT%H:%M:%S)</code></p>"},{"location":"UI/Install/","title":"Install","text":""},{"location":"UI/Install/#install-underpass-ui","title":"Install Underpass UI","text":"<pre><code>yarn add https://github.com/hotosm/underpass-ui.git\n</code></pre>"},{"location":"UI/Install/#import-styles-and-the-underpassmap-module","title":"Import styles and the UnderpassMap module","text":"<pre><code>import \"@hotosm/underpass-ui/dist/index.css\";\nimport { UnderpassMap } from \"@hotosm/underpass-ui\";\n</code></pre>"},{"location":"UI/Install/#example","title":"Example","text":"<pre><code>&lt;UnderpassMap\n    center={[44.39016, -89.79617].reverse()}\n    tags={\"building=yes\"}\n    zoom={18}\n/&gt;\n</code></pre>"},{"location":"UI/UnderpassFeatureList/","title":"UnderpassFeatureList","text":""},{"location":"UI/UnderpassFeatureList/#underpassfeaturelist","title":"UnderpassFeatureList","text":"<pre><code>&lt;UnderpassFeatureList \n    tags=\"building=yes\"\n    page={0}\n    hashtags=\"hotosm\"\n/&gt;\n</code></pre>"},{"location":"UI/UnderpassMap/","title":"UnderpassMap","text":""},{"location":"UI/UnderpassMap/#underpassmap","title":"UnderpassMap","text":"<pre><code>&lt;UnderpassMap\n    center={[44.39016, -89.79617].reverse()}\n    tags={\"building=yes,destroyed:building=yes\"}\n    zoom={18}\n/&gt;\n</code></pre>"},{"location":"get-started/Build/","title":"Build from source","text":""},{"location":"get-started/Build/#linux","title":"Linux","text":""},{"location":"get-started/Build/#install-dependencies","title":"Install dependencies","text":"<pre><code>sudo apt-get update \\\n    &amp;&amp; apt-get install -y software-properties-common \\\n    &amp;&amp; apt-get update &amp;&amp; apt-get install -y \\\n        libboost-dev \\\n        autotools-dev \\\n        swig \\\n        pkg-config \\\n        gcc \\\n        build-essential \\\n        ccache \\\n        libboost-all-dev \\\n        dejagnu \\\n        libjemalloc-dev \\\n        libxml++2.6-dev \\\n        doxygen \\\n        libgdal-dev \\\n        libosmium2-dev \\\n        libpqxx-dev \\\n        postgresql \\\n        libgumbo-dev \\\n        librange-v3-dev\n</code></pre>"},{"location":"get-started/Build/#build","title":"Build","text":"<pre><code>$ ./autogen.sh &amp;&amp; \\\n  mkdir build &amp;&amp; cd build &amp;&amp; \\ \n  ../configure &amp;&amp; make -j$(nproc) &amp;&amp; sudo make install\n</code></pre>"},{"location":"get-started/Build/#macos","title":"MacOS","text":""},{"location":"get-started/Build/#install-dependencies_1","title":"Install dependencies","text":"<pre><code>sudo port install boost\nbrew install \\\n    libtool \\\n    gdal \\\n    pkg-config \\\n    openssl \\\n    protobuf \\\n    boost-python3 \\\n    libxml++3 \\\n    libpqxx \\\n    gumbo-parser\n</code></pre>"},{"location":"get-started/Build/#build-intel","title":"Build (Intel)","text":"<pre><code>sh ./autogen.sh\nmkdir build &amp;&amp; cd build\n../configure CXXFLAGS=\" \\\n    -I/usr/local/include \\\n    -L/opt/homebrew/lib \\\n    -I/opt/homebrew/include \\\n    -I/opt/homebrew/Cellar \\\n    -L/usr/local/Cellar\" CXX=\"g++\"\nmake -j$(nproc) &amp;&amp; sudo make install\n</code></pre>"},{"location":"get-started/Build/#macos-sillicon","title":"MacOS (Sillicon)","text":"<p>The process is similar to Intel but adding <code>-arch arm64</code> to <code>CXXFLAGS</code>:</p> <pre><code>sh ./autogen.sh\nmkdir build &amp;&amp; cd build\n../configure CXXFLAGS=\" -arch arm64\\\n    -I/usr/local/include \\\n    -L/opt/homebrew/lib \\\n    -I/opt/homebrew/include \\\n    -I/opt/homebrew/Cellar \\\n    -L/usr/local/Cellar\" CXX=\"g++\"\nmake -j$(nproc) &amp;&amp; sudo make install\n</code></pre>"},{"location":"get-started/Install/","title":"Install","text":""},{"location":"get-started/Install/#docker","title":"Docker","text":"<p>If you want to get started really quick and easy, use the Docker installation</p>"},{"location":"get-started/Install/#build-from-source","title":"Build from source","text":"<p>Check the build documentation</p>"},{"location":"get-started/Install/#linux","title":"Linux","text":"<p><code>setup/install.sh</code> <code>apt-get install underpass</code> (not available yet)</p>"},{"location":"get-started/Install/#macos","title":"MacOS","text":"<p><code>brew install underpass</code> (not available yet)</p>"},{"location":"get-started/Install/#intel","title":"Intel","text":"<p><code>setup/install-macos.sh</code></p>"},{"location":"get-started/Install/#sillicon","title":"Sillicon","text":"<p><code>setup/install-macos-arm64.sh</code></p>"},{"location":"get-started/NextSteps/","title":"Next steps","text":""},{"location":"get-started/NextSteps/#next-steps","title":"Next steps","text":"<p>Once you have Underpass installed and you filled its database with data you might want:</p> <ul> <li>Keep the database up-to-date with OSM</li> <li>Get data from the DB using the API</li> <li>Display data on a browser using the UI</li> </ul>"},{"location":"get-started/Run/","title":"Run","text":""},{"location":"get-started/Run/#keep-the-database-up-to-date-minutely","title":"Keep the database up-to-date minutely","text":""},{"location":"get-started/Run/#linux","title":"Linux","text":"<p>Run this command for start processing data from 2 days ago:</p> <p><code>underpass -t $(date +%Y-%m-%dT%H:%M:%S -d \"2 days ago\")</code></p>"},{"location":"get-started/Run/#macos","title":"MacOS","text":"<p>On MacOS, the date command is slighty different:</p> <p><code>underpass -t $(date -v -2d +%Y-%m-%dT%H:%M:%S)</code></p>"},{"location":"get-started/Run/#docker","title":"Docker","text":"<p>If you're running Underpass on a Docker container:</p> <p><code>docker exec underpass underpass -t $(date +%Y-%m-%dT%H:%M:%S -d \"2 days ago\")</code></p> <p>For running as a daemon:</p> <p><code>docker exec -d underpass underpass -t $(date -v -2d +%Y-%m-%dT%H:%M:%S)</code></p>"},{"location":"get-started/Setup/","title":"Setup","text":""},{"location":"get-started/Setup/#boostraping-the-database","title":"Boostraping the database","text":"<p>You can prepare your Underpass installation with data for a specific country.</p>"},{"location":"get-started/Setup/#pre-requisites","title":"Pre-requisites","text":""},{"location":"get-started/Setup/#database","title":"Database","text":"<p>Prepare your PostgreSQL + PostGIS database, for example:</p> <pre><code>sudo apt update \nsudo apt install postgis\nsudo su - postgres\npsql\npostgres=# CREATE USER underpass WITH PASSWORD 'your_password';\npostgres=# CREATE DATABASE underpass;\npostgres=# GRANT ALL PRIVILEGES ON DATABASE \"underpass\" to underpass;\npostgres=# ALTER ROLE underpass SUPERUSER;\npostgres=# exit\nexit\npsql postgresql://underpass:your_password@localhost:5432/underpass &lt; setup/db/underpass.sql\n</code></pre>"},{"location":"get-started/Setup/#requirements","title":"Requirements","text":"<pre><code>sudo apt install python3-pip -y\nsudo apt install python3.11-venv\npython3 -m venv ~/venv\nsource ~/venv/bin/activate\npip install fiona\npip install shapely\napt install osm2pgsql\n</code></pre>"},{"location":"get-started/Setup/#bootstrap","title":"Bootstrap","text":"<p>Go to the <code>setup</code> directory and run the boostrap script:</p> <pre><code>cd utils\nchmod +x bootstrap.sh\n./bootstrap.sh -r south-america -c uruguay\n</code></pre> <p>Use <code>-u &lt;USERNAME&gt;</code> <code>-h &lt;HOST&gt;</code> <code>-d &lt;DATABASE&gt;</code> <code>-d &lt;PORT&gt;</code> for the database connection.</p> <p>If you installed Underpass with Docker, you might use the <code>-p 5439 -k yes</code> options.</p> <p><code>./bootstrap.sh -r south-america -c uruguay -p 5439 -k yes</code></p> <p>Regions (-r) are:     africa     asia     australia-oceania     central-america     europe     north-america     south-america</p> <p>Countries (-c) is the name of the country inside the region.</p> <p>Data is downloaded from GeoFabrik, if you are not sure of what name you need to use, please check there.</p> <p>For advanced users, check the boostrap script documentation.</p>"}]}